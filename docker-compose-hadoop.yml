services:
  namenode:
    build: ./namenode
    container_name: namenode
    volumes:
      - /bigdata/hadoop_namenode:/hadoop/dfs/name
      - ./data/:/hadoop-data/input
      - ./map_reduce/:/hadoop-data/map_reduce
      - ./requirements.txt:/hadoop-data/requirements.txt
    environment:
      - CLUSTER_NAME=development
    env_file:
      - ./hadoop.env
    ports:
      - 9870:9870
      - 8020:8020
    networks:
      - hadoop_network

  resourcemanager:
    build: ./resourcemanager
    container_name: resourcemanager
    restart: on-failure
    depends_on:
      - namenode
      - datanode1
      - datanode2
      - datanode3
    env_file:
      - ./hadoop.env
    ports:
      - 8089:8088
    networks:
      - hadoop_network

  historyserver:
    build: ./historyserver
    container_name: historyserver
    depends_on:
      - namenode
      - datanode1
      - datanode2
    volumes:
      - /bigdata/hadoop_historyserver:/hadoop/yarn/timeline
    env_file:
      - ./hadoop.env
    ports:
      - 8188:8188
    networks:
      - hadoop_network

  nodemanager1:
    build: ./nodemanager
    container_name: nodemanager1
    depends_on:
      - namenode
      - datanode1
      - datanode2
    env_file:
      - ./hadoop.env
    volumes:
      - /bigdata/hadoop_namenager_local:/hadoop/dfs/local
      - /bigdata/hadoop_namenager_logs:/hadoop/dfs/logs
    ports:
      - 8042:8042
    networks:
      - hadoop_network

  # nodemanager2:
  #   build: ./nodemanager
  #   container_name: nodemanager2
  #   depends_on:
  #     - resourcemanager
  #     - namenode
  #     - datanode1
  #     - datanode2
  #   env_file:
  #     - ./hadoop.env
  #   volumes:
  #     - /bigdata/hadoop_namenager_local:/hadoop/dfs/local
  #     - /bigdata/hadoop_namenager_logs:/hadoop/dfs/logs
  #   ports:
  #     - 8043:8043
  #   networks:
  #     - hadoop_network

  datanode1:
    build: ./datanode
    container_name: datanode1
    depends_on:
      - namenode
    volumes:
      - /bigdata/hadoop_datanode1:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    networks:
      - hadoop_network

  datanode2:
    build: ./datanode
    container_name: datanode2
    depends_on:
      - namenode
    volumes:
      - /bigdata/hadoop_datanode2:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    networks:
      - hadoop_network

  datanode3:
     build: ./datanode
     container_name: datanode3
     depends_on:
       - namenode
     volumes:
       - /bigdata/hadoop_datanode3:/hadoop/dfs/data
     env_file:
       - ./hadoop.env
     networks:
       - hadoop_network
  
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    env_file:
      - ./hadoop.env
    environment:
      HIVE_CORE_CONF_javax_jdo_option_ConnectionURL: "jdbc:postgresql://hive-metastore/metastore"
      SERVICE_PRECONDITION: "hive-metastore:9083"
    ports:
      - "10000:10000"

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop.env
    command: /opt/hive/bin/hive --service metastore
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode:50075 hive-metastore-postgresql:5432 resourcemanager:8088"
    ports:
      - "9083:9083"

  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgres
    ports:
      - "5434:5432"

  huedb:
    image: postgres:12.1-alpine
    container_name: huedb
    volumes:
      - /bigdata/hue_data:/var/lib/postgresl/data/
    ports:
      - "5433"
    env_file:
      - ./hadoop.env
    environment:
        SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 datanode3 hive-metastore-postgresql:5432 resourcemanager:8088 hive-metastore:9083"
  
  hue:
    image: gethue/hue:4.6.0
    container_name: hue
    environment:
        SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 datanode3:9864 hive-metastore-postgresql:5432 resourcemanager:8088 hive-metastore:9083 huedb:5000"
    ports:
      - "8888:8888"
    volumes:
      - ./hue-overrides.ini:/usr/share/hue/desktop/conf/hue-overrides.ini
    links:
      - huedb
  # hive-metastore:
  #   build: ./hive/hive-metastore
  #   restart: always
  #   container_name: hive-metastore
  #   logging:
  #     driver: "json-file"
  #     options:
  #         max-file: "5"
  #         max-size: "10m"
  #   # depends_on:
  #   #   - namenode
  #   #   - datanode
  #   #   - postgres
  #   environment:
  #     - SERVICE_PRECONDITION=namenode:9870 datanode1:9864 datanode2:9864 datanode3:9864 postgres:5432
  #   ports:
  #     - "32761:9083"
  #   healthcheck:
  #     test: [ "CMD", "nc", "-z", "hive-metastore", "9083" ]
  #     timeout: 45s
  #     interval: 10s
  #     retries: 10
  #   networks:
  #     - hadoop_network

  # hive-server:
  #   build: ./hive/hive-server
  #   restart: always
  #   container_name: hive-server
  #   logging:
  #     driver: "json-file"
  #     options:
  #         max-file: "5"
  #         max-size: "10m"
  #   depends_on:
  #     - hive-metastore
  #   environment:
  #     - SERVICE_PRECONDITION=hive-metastore:9083
  #   ports:
  #     - "32760:10000"
  #     - "32759:10002"
  #   healthcheck:
  #     test: [ "CMD", "nc", "-z", "hive-server", "10002" ]
  #     timeout: 45s
  #     interval: 10s
  #     retries: 10
  #   networks:
  #     - hadoop_network

  # hive-webhcat:
  #   build: ./hive/hive-webhcat
  #   restart: always
  #   container_name: hive-webhcat
  #   logging:
  #     driver: "json-file"
  #     options:
  #         max-file: "5"
  #         max-size: "10m"
  #   depends_on:
  #     - hive-server
  #   environment:
  #     - SERVICE_PRECONDITION=hive-server:10000
  #   healthcheck:
  #     test: [ "CMD", "nc", "-z", "hive-webhcat", "50111" ]
  #     timeout: 45s
  #     interval: 10s
  #     retries: 10
  #   networks:
  #     - hadoop_network

  # hue:
  #   build: ./hue
  #   restart: always
  #   container_name: hue
  #   logging:
  #     driver: "json-file"
  #     options:
  #         max-file: "5"
  #         max-size: "10m"
  #   # depends_on:
  #   #   - hive-server
  #   #   - postgres
  #   ports:
  #     - "3276:8888"
  #   volumes:
  #     - /bigdata/hue_data:/usr/share/hue/desktop
  #   environment:
  #     - SERVICE_PRECONDITION=hive-server:10000 postgres:5432
  #   healthcheck:
  #     test: [ "CMD", "nc", "-z", "hue", "8888" ]
  #     timeout: 45s
  #     interval: 10s
  #     retries: 10
  #   networks:
  #     - hadoop_network

  # livy:
  #   build: ./livy
  #   restart: always
  #   container_name: livy
  #   logging:
  #     driver: "json-file"
  #     options:
  #         max-file: "5"
  #         max-size: "10m"
  #   # depends_on:
  #   #   - spark-worker-a
  #   #   - spark-worker-b
  #   ports:
  #       - "32758:8998"
  #   environment:
  #       - SPARK_MASTER_ENDPOINT=spark-master
  #       - SPARK_MASTER_PORT=7077
  #       - DEPLOY_MODE=client
  #   healthcheck:
  #       test: [ "CMD", "nc", "-z", "livy", "8998" ]
  #       timeout: 45s
  #       interval: 10s
  #       retries: 10
  #   networks:
  #     - hadoop_network

# volumes:
#   hadoop_namenode:
#   hadoop_datanode1:
#   hadoop_datanode2: 
#   hadoop_datanode3:
#   hadoop_historyserver:

# volumes:
#   hadoop_namenode:
#           driver: local-persist
#           driver_opts:
#                mountpoint: /bigdata/hadoop_namenode
#   hadoop_datanode1:
#           driver: local-persist
#           driver_opts:
#                mountpoint: /bigdata/hadoop_datanode1
#   hadoop_datanode2:
#           driver: local-persist
#           driver_opts:
#                mountpoint: /bigdata/hadoop_datanode2
#   hadoop_datanode3:
#           driver: local-persist
#           driver_opts:
#                mountpoint: /bigdata/hadoop_datanode3
#   hadoop_historyserver:
#           driver: local-persist
#           driver_opts:
#                mountpoint: /bigdata/hadoop_historyserver

networks:
  hadoop_network:
    name: hadoop_network
    external: true