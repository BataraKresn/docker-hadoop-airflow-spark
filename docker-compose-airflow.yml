x-airflow-common: &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-airflow-hadoop-base:3.3.6}
  environment:
    AIRFLOW_HOME: /home/airflow
    AIRFLOW__CORE_dags_folder: /home/airflow/dags
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/0
    AIRFLOW__CELERY__RESULT_BACKEND: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    AIRFLOW__CORE__XCOM_BACKEND: airflow.providers.redis.xcom.RedisXCom
    AIRFLOW__CORE__FERNET_KEY: FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
    AIRFLOW__CORE__LOAD_EXAMPLES: False
    RATELIMIT_STORAGE_URL: redis://redis:6379/0
    _PIP_ADDITIONAL_REQUIREMENTS: "pendulum apache-airflow-providers-redis==3.6.1 apache-airflow-providers-celery==3.3.1 apache-airflow-providers-fab==1.0.3"
    # - TZ=Asia/Jakarta
  volumes:
    - /bigdata/airflow/output:/home/airflow/output
    - ./dags:/home/airflow/dags
    - /bigdata/airflow/plugins:/home/airflow/plugins
    - /bigdata/airflow/logs:/home/airflow/logs
    - ./data/:/hadoop-data/input
    - ./map_reduce/:/hadoop-data/map_reduce
    - ./airflow/airflow.cfg:/home/airflow.cfg
  depends_on:
    - postgres
    - redis
  networks:
    - hadoop_network

services:
  postgres:
      image: postgres:10.3
      container_name: postgres
      environment:
        # - POSTGRES_USER=airflow
        # - POSTGRES_PASSWORD=airflow
        # - POSTGRES_DB=airflow
        # - POSTGRES_HOST_AUTH_METHOD=trust
        - POSTGRES_USER=postgres
        - POSTGRES_PASSWORD=postgres
        - POSTGRES_DB=postgres
        # - TZ=Asia/Jakarta
      ports:
        - 5432:5432
      volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - /bigdata/postgres/data:/var/lib/postgresql/data
      - /bigdata/postgres/logs:/var/lib/postgresql/log
      - ./postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf
      - ./postgres/01-postgres-init.sql:/docker-entrypoint-initdb.d/01-postgres-init.sql
      - ./postgres/02-hive-schema-2.3.0.postgres.sql:/docker-entrypoint-initdb.d/02-hive-schema-2.3.0.postgres.sql
      - ./postgres/03-hive-txn-schema-2.3.0.postgres.sql:/docker-entrypoint-initdb.d/03-hive-txn-schema-2.3.0.postgres.sql
      networks:
        - hadoop_network

  pgadmin:
      image: dpage/pgadmin4
      container_name: pgadmin
      environment:
        - PGADMIN_DEFAULT_EMAIL=bigdata0124@gmail.com
        - PGADMIN_DEFAULT_PASSWORD=3J1}/Ux82Y5;
      ports:
        - 5000:80 # Expose pgAdmin port for external access
      volumes:
        - /bigdata/postgres/pgadmin_data:/var/lib/pgadmin
      depends_on:
        - postgres
      networks:
        - hadoop_network

  redis:
    image: redis:5.0.2
    container_name: redis
    # environment:
    #   - REDIS_HOST: redis
    #   - REDIS_PORT: 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 30s
      retries: 5
    expose:
      - 6379
    volumes:
      - /bigdata/redis/data:/data
    restart: always
    networks:
      - hadoop_network

  airflow_init:
    <<: *airflow-common
    container_name: airflow_init
    entrypoint: /bin/bash
    command:
      - -c
      - airflow db init && airflow db upgrade && airflow users create --role Admin --username airflow --password airflow --email airflow@airflow.com --firstname airflow --lastname user
    restart: on-failure
    networks:
      - hadoop_network

  airflow_webserver:
    <<: *airflow-common
    command: airflow webserver
    ports:
      - 8080:8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    container_name: airflow_webserver
    depends_on:
      - airflow_init
      - postgres
      - redis
    restart: always
    networks:
      - hadoop_network

  airflow_scheduler:
    <<: *airflow-common
    command: airflow scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    container_name: airflow_scheduler
    depends_on:
      - airflow_init
      - redis
    restart: always
    networks:
      - hadoop_network

  airflow_flower:
    <<: *airflow-common
    command: airflow flower
    container_name: airflow_flower
    ports:
      - "5555:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      - redis
    networks:
      - hadoop_network

  airflow_triggerer:
    <<: *airflow-common
    command: triggerer
    container_name: airflow_triggerer
    restart: always
    depends_on:
      - redis
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5
    networks:
      - hadoop_network

  airflow_worker:
    <<: *airflow-common
    command: celery worker
    container_name: airflow_worker
    healthcheck:
      test:
        - "CMD-SHELL"
        - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 10s
      timeout: 10s
      retries: 5
    depends_on:
      - airflow_scheduler
    networks:
      - hadoop_network
  
  airflow_cli:
    <<: *airflow-common
    container_name: airflow_cli
    command: /bin/bash
    depends_on:
      - redis
    networks:
      - hadoop_network

networks:
  hadoop_network:
    name: hadoop_network
    external: true